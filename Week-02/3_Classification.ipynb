{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455be52c",
   "metadata": {},
   "source": [
    "# Week 2 - Classical ML Models - Part I\n",
    "\n",
    "## 3. Classification\n",
    "\n",
    "As it has been mentioned in the previous week, in the classification problems we have a set of inputs that belong to 2 or more categories and we have to train model to assign new set of inputs to corresponding categories.\n",
    "\n",
    "Although there are many existing classification models, in this lecture we will focus on the **logistic regression**.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Even though it includes the regression term, it is more related to classification than regression models. Logistic regression is essentially used to calculate the probability of a binary event.\n",
    "\n",
    "Before going too much in-depth to the logistic regression model, we will first analyze its *'building blocks'*.\n",
    "\n",
    "#### Sigmoid function\n",
    "\n",
    "![sigmoid function](https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png)\n",
    "\n",
    "The sigmoid function (seen above), maps any real values to a range of 0 and 1. As we train, logistic regression model, we are basically aiming to find the threshold value: the values above threshold is considered as 1, while values below threshold are 0.\n",
    "\n",
    "The sigmoid function itself can be expressed mathematically as\n",
    "$g(z) = \\frac{1}{1 + \\exp(-z)}$. In Python code, this could be expressed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde95c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e52d9",
   "metadata": {},
   "source": [
    "#### Logistic regression hypothesis\n",
    "\n",
    "So far we know how to express the sigmoid function in mathematical and code notations. We also know that in the logistic regression the inputs are passed through the sigmoid in order to determine the classification threshold value.\n",
    "\n",
    "However, how does this connection translate in mathematics?\n",
    "\n",
    "As you might remember from the last notebook, the linear regression had hypothesis $ \\hat{y} = \\theta ^T x$. For the logistic regression, this hypothesis becomes: $ \\hat{y} = \\frac{1}{1 + \\exp(-\\theta ^T x)}$.\n",
    "\n",
    "The hypothesis can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05665d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    y_hat = sigmoid(z)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8235187",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "\n",
    "At this point, we have our function for our logistic function probability output. On the other hand, in order to actually train our model, we also have to define our cost function.\n",
    "\n",
    "In the linear regression case, our cost function had the following form:\n",
    "$\\frac{1}{m} \\sum_{i = 1}^m(\\hat{y_{i}} - y_{i}) ^ 2$. Here $\\hat{y_{i}}$ is the output of our probability function while $y_{i}$ is the actual label. On the other hand, our probability function $\\hat{y}$ has a more complicated expression that would make it quite hard to find its optimum (function would have many local minimum points). Therefore, the cost function for the logistic regression can be written as: $J(\\theta) = -\\frac{1}{m} \\sum_{i = 1}^m (y_{i} \\log{(\\hat{y}_{i})} + (1-y_{i}) \\log{(1 - \\hat{y}_{i})})$.\n",
    "\n",
    "In code this can be expressed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20197fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat, y):\n",
    "    cost = np.mean(y * np.log(y_hat) + (1 - y) * np.log(1-y_hat))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efe027",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "\n",
    "As it has been mentioned in the previous notebook, the optimization process in ML models is usually based on the gradient descent algorithm. We will analyze it much more in depth in the near future lectures, however, just for the purpose of this tutorial, understand it as a way of finding the minimum of the function. As you might remember from the math lessons, the minimum point can be found with the use of derivatives. As we want to analyze the cost function in respect to $\\theta$, we need to differentiate our cost function: $\\frac{1}{m} X^T (\\hat{y} - y)$.\n",
    "\n",
    "The gradient descent formula for updatint $\\theta$ values, therefore, becomes:\n",
    "$\\theta = \\theta - lr.\\frac{1}{m} X^T (\\hat{y} - y)$\n",
    "\n",
    "The code for finding the optimal $\\theta$ becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_train, y_train, lr, epochs):\n",
    "    intercept = np.ones((x_train.shape[0], 1))\n",
    "    x_train = np.concatenate((intercept, x_train), axis=1)\n",
    "    theta = np.zeros(x_train.shape[1])\n",
    "\n",
    "    m = len(x_train)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        y_hat = hypothesis(x_train, theta)\n",
    "        gradient = np.dot(x_train.T, (y_hat - y_train)) / m\n",
    "        \n",
    "        theta -= lr * gradient\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a200c",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "After finding the optimal $\\theta$, the prediction process is quite straightforward - we just simply need to make a new hypothesis. The output of such function ($\\hat{y}$) will be a probability in the range from $0$ to $1$, therefore, we need to map values according to $0.5$ threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6baf7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_train, y_train, x_test, lr, epochs):\n",
    "    \n",
    "    theta = gradient_descent(x_train, y_train)\n",
    "    intercept = np.ones((x_test.shape[0], 1))\n",
    "    x_test = np.concatenate((intercept, x_test), axis=1)\n",
    "    y_hat = hypothesis(x_test, theta)\n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(len(y_hat)):\n",
    "        if(y_hat[i] >= 0.5):\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25a0ef",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's now put it all together into a simple logistic model. For this example exercise, we will use one of the sklearn datasets (Iris). Do not change the cell below containing data importation - **only write code to the cells with comments**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "28e20aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############-------Do not change this cell-------##########################\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "#Features\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee00a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "############-------Tasks to do-------------------##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the X, y dataset into x_train, x_test, y_train and y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Building the model\n",
    "\n",
    "#sigmoid function\n",
    "\n",
    "\n",
    "#hypothesis function\n",
    "\n",
    "\n",
    "#cost function\n",
    "\n",
    "\n",
    "#gradient descent function\n",
    "\n",
    "\n",
    "#prediction function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning learning rate and epochs values (can leave as it is or change it)\n",
    "lr = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting values\n",
    "y_pred =       #here should be your prediction function\n",
    "print('Accuracy on test set: ' + str(accuracy_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
