{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2db0cc9",
   "metadata": {},
   "source": [
    "# 3. Bidirectional Recurrent Neural Networks (BRNN)\n",
    "## Limitation of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8c714",
   "metadata": {},
   "source": [
    "In the last lecture, we learned about **Recurrent Neural Networks**. What distinguishes itself from feedforward layers is that it can process sequential data. Although it sounds super cool enough, there's also limitation to it.  \n",
    "\n",
    "Let's say we want to build a model that predicts which word would fall into the blank, `[ ]`. What would be the answer in the following sentence?\n",
    "\n",
    "- He said, \"Teddy `[ ]`\"\n",
    "\n",
    "Yes, it's pretty hard to predict the next word since we only have the past data, which are words before the blank. And how would you know if \"Teddy\" means a Teddy bear, Teddy Roosevelt, Teddy Sears, or the new Indian action thriller [film](https://en.wikipedia.org/wiki/Teddy_(film)) secretly released on 2021? It seems it might be more helpful to know which words come after the blank. The problem of RNNs is that it misses out future information which might hold important context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d6afa7",
   "metadata": {},
   "source": [
    "## BRNN is here\n",
    "Here comes Bidirectional Recurrent Neural Networks. As the name suggests, it not only holds a memory of inputs in one order that RNNs does, but the other way around. This way, the output layer can receive more rich information from both directions, which makes it outstanding especially when the context of the input should be considered.\n",
    "\n",
    "Let's get back to the previous example to intuitively understand why it's awesome. We have the same task to predict the word in the blank, but this time, we are also given next words.\n",
    "- He said, \"Teddy `[ ]` are cuter than you.\"\n",
    "\n",
    "Now, it makes sense. The word in the blank should be \"Roosevelt.\"\n",
    "\n",
    "You spotted it's a joke for sure because you also looked into what comes after the blank. BRNN does the same. \n",
    "\n",
    "Be careful about when to use BRRN, though. As you might have guessed, we are not always guaranteed to look into future. That half of its structure depends on the future data means that it can perform half poorly when it has no access to one. \n",
    "\n",
    "Still, it makes perfect sense to use Bidirectional RNN in various areas where we're likely to have complete inputs such as\n",
    "\n",
    "- Speech Recognition\n",
    "- Translation\n",
    "- Handwrrting Recognition\n",
    "- Protein Structure Prediction\n",
    "- Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b36d7",
   "metadata": {},
   "source": [
    "## BRNN's Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684074e",
   "metadata": {},
   "source": [
    "![DIVE INTO DEEP LEARNING](https://d2l.ai/_images/birnn.svg)\n",
    "<center><i>BRNN Image from DIVE INTO DEEP LEARNING</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f66301",
   "metadata": {},
   "source": [
    "BRNN is largely composed of two vertically stacked RNNs. \n",
    "\n",
    "$X_t \\in \\mathbb{R}^{n \\times d}$ (where $n$ is the number of sequences and $d$ is the number of input features) is an input for each time step. And let $\\phi$ be an activation function for hidden layer.\n",
    "\n",
    "In the forward layer, $\\overrightarrow{\\text{H}_t}$ passes its output to both its next time step and the output. \n",
    "\n",
    "$$\\overrightarrow{\\text{H}_t} = \\phi(X_t\\overrightarrow{\\text{W}}_{xh} + \\overrightarrow{\\text{H}}_{t-1}\\overrightarrow{\\text{W}}_{hh} + \\overrightarrow{b_h})$$\n",
    "\n",
    "The layer above $\\overleftarrow{\\text{H}_t}$ does it backward.\n",
    "\n",
    "$$\\overleftarrow{\\text{H}_t} = \\phi(X_t\\overleftarrow{\\text{W}}_{xh} + \\overleftarrow{\\text{H}}_{t+1}\\overleftarrow{\\text{W}}_{hh} + \\overleftarrow{b_h})$$\n",
    "\n",
    "We can obtain the output $O_t$ from the concatenation of the two outputs of $\\overrightarrow{\\text{H}}_t$ and $\\overleftarrow{\\text{H}}_t$.\n",
    "\n",
    "$$O_t = H_{t}W_{ho} + b_o$$\n",
    "\n",
    "where $W_{ho} \\in \\mathbb{R}^{2h \\times o} $ denotes the weights between the hidden layers and the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0871b",
   "metadata": {},
   "source": [
    "## Keras implementation\n",
    "\n",
    "After covering the theory, let's now look how BRNNs can be implemented using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737d355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True, input_shape = (5, 10))),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1800e",
   "metadata": {},
   "source": [
    "As you can see, the `Bidirectional()` layer function handles the structuring of our neural network and the only thing we are required to do is specifying the type of recurrent network we want to use for our Bidirectional layer. In the example case, we used LSTM, but you could use RNN or GRU (we will be covering it in the future tutorials)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
